{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3494,"sourceType":"datasetVersion","datasetId":2050}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom transformers import BertTokenizer, BertForSequenceClassification\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.tensorboard import SummaryWriter\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntrain_data = pd.read_csv('/kaggle/working/train.csv')\ntest_data = pd.read_csv('/kaggle/working/test.csv')\ntrain_data['Category'] = train_data['Category'].map({'spam': 1, 'ham': 0})  # Assuming 'spam' is 1 and 'ham' is 0\ntest_data['Category'] = test_data['Category'].map({'spam': 1, 'ham': 0})\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\nclass CustomDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=128):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        text = self.data.iloc[idx]['Message']\n        label = self.data.iloc[idx]['Category']\n        \n        inputs = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': inputs['input_ids'].flatten(),\n            'attention_mask': inputs['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\ntrain_dataset = CustomDataset(train_data, tokenizer)\ntest_dataset = CustomDataset(test_data, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=False).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=2e-5)\n\nwriter = SummaryWriter('logs')\n\ndef train(model, loader, optimizer, criterion, epoch, writer):\n    model.train()\n    epoch_loss = 0\n    correct = 0\n    total = 0\n    \n    for batch_idx, batch in enumerate(loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        optimizer.zero_grad()\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs[0]\n        logits = outputs[1]\n        \n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        _, predicted = torch.max(logits, 1)\n        correct += (predicted == labels).sum().item()\n        total += labels.size(0)\n\n    accuracy = correct / total\n\n    writer.add_scalar('Loss/train', epoch_loss / len(loader), epoch)\n    writer.add_scalar('Accuracy/train', accuracy, epoch)\n\n    return epoch_loss / len(loader), accuracy\n\ndef evaluate(model, loader, criterion, epoch, writer):\n    model.eval()\n    epoch_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for batch_idx, batch in enumerate(loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs[0]\n            logits = outputs[1]\n            \n            epoch_loss += loss.item()\n            _, predicted = torch.max(logits, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n    accuracy = correct / total\n\n    writer.add_scalar('Loss/test', epoch_loss / len(loader), epoch)\n    writer.add_scalar('Accuracy/test', accuracy, epoch)\n\n    return epoch_loss / len(loader), accuracy\n\nN_EPOCHS = 5\n\nfor epoch in range(N_EPOCHS):\n    train_loss, train_acc = train(model, train_loader, optimizer, criterion, epoch, writer)\n    test_loss, test_acc = evaluate(model, test_loader, criterion, epoch, writer)\n\n    print(f'Epoch: {epoch+1:02}')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\tTest Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n\nwriter.close()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-24T12:23:19.055835Z","iopub.execute_input":"2024-04-24T12:23:19.056197Z","iopub.status.idle":"2024-04-24T12:31:23.446187Z","shell.execute_reply.started":"2024-04-24T12:23:19.056166Z","shell.execute_reply":"2024-04-24T12:31:23.445248Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 01\n\tTrain Loss: 0.092 | Train Acc: 97.53%\n\tTest Loss: 0.041 | Test Acc: 99.01%\nEpoch: 02\n\tTrain Loss: 0.029 | Train Acc: 99.33%\n\tTest Loss: 0.027 | Test Acc: 99.46%\nEpoch: 03\n\tTrain Loss: 0.012 | Train Acc: 99.64%\n\tTest Loss: 0.028 | Test Acc: 99.19%\nEpoch: 04\n\tTrain Loss: 0.007 | Train Acc: 99.82%\n\tTest Loss: 0.043 | Test Acc: 99.01%\nEpoch: 05\n\tTrain Loss: 0.002 | Train Acc: 99.93%\n\tTest Loss: 0.032 | Test Acc: 99.19%\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv('/kaggle/input/spam-text-message-classification/SPAM text message 20170820 - Data.csv')\n\ntrain_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n\ntrain_data.to_csv('train.csv', index=False)\ntest_data.to_csv('test.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-24T12:20:06.029306Z","iopub.execute_input":"2024-04-24T12:20:06.029950Z","iopub.status.idle":"2024-04-24T12:20:06.101617Z","shell.execute_reply.started":"2024-04-24T12:20:06.029918Z","shell.execute_reply":"2024-04-24T12:20:06.100911Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/spam-text-message-classification/SPAM text message 20170820 - Data.csv\")\nunique_categories = data['Category'].unique()\nprint(unique_categories)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T12:32:43.203145Z","iopub.execute_input":"2024-04-24T12:32:43.203845Z","iopub.status.idle":"2024-04-24T12:32:43.222511Z","shell.execute_reply.started":"2024-04-24T12:32:43.203818Z","shell.execute_reply":"2024-04-24T12:32:43.221646Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"['ham' 'spam']\n","output_type":"stream"}]}]}